{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing from Llama-2 (7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers langchain accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========#\n",
    "# Set seed #\n",
    "#==========#\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import set_seed\n",
    "\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Huggingface and langchain for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_auth_key = open(\"./auth_keys/huggingface_auth_key.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import transformers\n",
    "from termcolor import colored\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM)\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "class LLaMA2ChatAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        model: AutoModelForCausalLM,\n",
    "        chat_history: ConversationBufferMemory = ConversationBufferMemory(memory_key=\"chat_history\"),\n",
    "        new_system_prompt: str = None,\n",
    "        max_tokens: int = 128,\n",
    "        temperature: float = 0.7,\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.llm = HuggingFacePipeline(\n",
    "            pipeline = transformers.pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model=self.model, tokenizer=self.tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "            ),\n",
    "            model_kwargs = {'temperature': self.temperature}\n",
    "        )\n",
    "        self.chat_history = chat_history\n",
    "        self.B_INST, self.E_INST = \"[INST]\", \"[/INST]\"\n",
    "        self.B_SYS, self.E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "        self.new_system_prompt = new_system_prompt\n",
    "        self.DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.chat_history.clear()\n",
    "\n",
    "    def get_prompt(self, instruction: str, new_system_prompt: str):\n",
    "        SYSTEM_PROMPT = self.B_SYS + new_system_prompt + self.E_SYS\n",
    "        prompt_template =  self.B_INST + SYSTEM_PROMPT + instruction + self.E_INST\n",
    "        return prompt_template\n",
    "\n",
    "    def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print( colored(wrapped_text, \"green\") +'\\n\\n')\n",
    "        # return assistant_text\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        template = self.get_prompt(\n",
    "            instruction=\"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}\",\n",
    "            new_system_prompt=self.DEFAULT_SYSTEM_PROMPT if self.new_system_prompt is None else self.new_system_prompt\n",
    "        )\n",
    "        prompt = PromptTemplate(input_variables=[\"chat_history\", \"user_input\"], template=template)\n",
    "        llm_chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=prompt,\n",
    "            verbose=True,\n",
    "            memory=self.chat_history,\n",
    "        )\n",
    "        return llm_chain.predict(user_input=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_ChatAgent = LLaMA2ChatAgent(\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=huggingface_auth_key),\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\", device_map='auto', torch_dtype=torch.float16, use_auth_token=huggingface_auth_key),\n",
    "    new_system_prompt = \"\"\"Respond with a response in the format requested by the user. Do not acknowledge my request with \"sure\" or in any other way besides going straight to the answer.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = open(\"./auth_keys/openai_key.txt\", \"r\").read()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
